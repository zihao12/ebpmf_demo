---
title: "investigate_mle_nb"
author: "zihao12"
date: "2020-03-05"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction
* I want to investigate the optimization of the MLE for the negative binomial model. 

* The model is  

\begin{align*}
    & x \sim NB(\text{size} = a, \text{prob} = p)\\
    & p(x) = \frac{\Gamma(x + a)}{\Gamma(x + 1) \Gamma(a)} p^a (1-p)^x\\
    & l(x; a, p) = \sum_{i = 1}^n [log \Gamma(x_i + a) + x_i log(1-p)] - n (log \Gamma(a) - a log p) - \sum_i log \Gamma(x_i + 1) 
\end{align*}

It is easy to see $p^{*}(a) = \frac{a}{a + \hat{x}}$. Then we optimize: 
\begin{align*}
    J(a) & := l(x; a , p^{*}(a))\\
    & = \sum_{i = 1}^n (log(x_i + a) - log(a)) + (n ( log a - log (a + \hat{x}))) a + const 
\end{align*}

## shapes of the objective function

### Second part
Note that if $x$ is a long and sparse sequence, the objective is close to trhe second part, $(n ( log a - log (a + \hat{x}))) a$, which looks like (let the variable be $r = log(a)$, and I will plot the negative of the approximate objective):
```{r}
## lat x_hat = 1, as it is just a matter of scaling
approx_obj <- function(a, xhat = 1){a*(log(a) - log(a + xhat))}
r_ = seq(-15, 15, 0.1)
plot(r_, - approx_obj(exp(r_)), type = "l")
```
It is convex on the right half of the support. So the initialization seems to matter a lot.  

### First part
Let's plot the negative of the first part of the objective
```{r}
first_obj <- function(a, x = 1){lgamma(a + x) - lgamma(a)}
r_ = seq(-15, 15, 0.1)

## x is almost constant
plot(r_, - first_obj(exp(r_)), type = "l")

## x has huge variance
plot(r_, - first_obj(exp(r_), x= 1) - first_obj(exp(r_), x= 1e+4), type = "l")
```
So the objective would be a combination of those shapes ... which can be quite non-convex and dependent on initialization. 


## some objective functions
```{r}
rm(list = ls())
library(Rfast)
library(stats)
source("~/Desktop/git/ebpm/R/mle_two_gamma5.R")
set.seed(123)
```

```{r}
simulate_gamma_poisson <- function(shape, scale, s = 1, n = 1000, seed = 123){
	set.seed(seed)
	lam = rgamma(n = n, shape = shape, scale = scale)
	x = rpois(n = n, lambda = s*lam)
	return(x)
}

show_obj <- function(shape, scale, s = 1, n = 1000, seed = 123, table_x = FALSE){
  x = simulate_gamma_poisson(shape, scale, s = s, n = n, seed = seed)
  w = replicate(n, 1/n)
  r_ = seq(-25, 25, 0.1)
  M = length(r_)
  obj = replicate(M, NaN)
  grad = replicate(M, NaN)
  hess = replicate(M, NaN)
  for(i in 1:M){
    tmp = obj.wnb.loga(r = r_[i], x = x, s = s, w = w, gradient = TRUE, hessian = TRUE)
    obj[i] = tmp
    grad[i] = attr(tmp, "gradient")
    hess[i] = attr(tmp, "hessian")
  }
  if(table_x){print(table(x))}
  par(mfrow = c(2,2))
  hist(log10(x + 1), breaks = 100)
  plot(r_,obj, xlab = "log(a)", ylab = "negative loglikelihood", type = "l")
  plot(r_,grad, xlab = "log(a)", ylab = "grad", type = "l")
  plot(r_,hess, xlab = "log(a)", ylab = "hess", type = "l")
}
```


```{r}

show_obj(shape = 50, scale = 0.1)
show_obj(shape = 50, scale = 1)
show_obj(shape = 50, scale = 10)
show_obj(shape = 0.5, scale = 1)
show_obj(shape = 0.5, scale = 10)

show_obj(shape = 0.5, scale = 0.1, table_x = TRUE)
show_obj(shape = 1e-2, scale = 1e+3, table_x = TRUE)
show_obj(shape = 1e-3, scale = 1e+4, table_x = TRUE)
show_obj(shape = 1e-4, scale = 1e+5, table_x = TRUE)
show_obj(shape = 1e-6, scale = 1e+7, table_x = TRUE)
```


