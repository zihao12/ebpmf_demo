---
title: "debug_ebpmf_exponential_mixture"
author: "zihao12"
date: "2019-10-22"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

I managed to find the bug in `ebpmf_exponential_mixture`. Now we can make `ELBO` increase monotonically if we fix the scale/grid. When we unfix the grid, the `ELBO` also increase monotonically if the grids chosen are dense enough. (As suggested by [Jason](https://github.com/willwerscheid), setting $m = 2^{0.25}$ works). 

Another thing that's unexpected is: rank-1 fit gets better RMSE than rank-K fit (where K is truth) in  both MLE and EBPMF. 

```{r warning = F}
rm(list  = ls())
devtools::load_all("../ebpmf")
set.seed(123)
library(NNLM)
library(ebpmf)

simulate_data <- function(n, p, K, params, seed = 1234){
  set.seed(seed)
  L = matrix(rgamma(n = n*K, shape = params$al, rate = params$bl), ncol = K)
  F = matrix(rgamma(n = p*K, shape = params$af, rate = params$bf), ncol = K)
  Lam =  L %*% t(F)
  X = matrix(rpois(n*p, Lam), nrow = n)
  Y = matrix(rpois(n*p, Lam), nrow = n)
  return(list(params = params,Lam = Lam,X = X, Y = Y))
}
n = 100
p = 200
K = 3
params = list(al = 100, bl =  109, af = 100, bf = 100, a = 1)
sim = simulate_data(n, p, K, params, seed =  1234)

maxiter = 100
```

## rank 1

### mle
```{r message=F, warning=F}
## MLE
mle_rank1 = NNLM::nnmf(A = sim$X, k = 1, method = "lee", loss = "mkl", max.iter = 1)
lam_mle_rank1 =  mle_rank1$W %*% mle_rank1$H
ll_train_mle_rank1  = sum(dpois(sim$X, lam_mle_rank1, log = T))
ll_val_mle_rank1  = sum(dpois(sim$Y, lam_mle_rank1, log = T))
rmse_mle_rank1 = sqrt(mean((lam_mle_rank1 - sim$Lam)^2))
data.frame(ll_train = ll_train_mle_rank1,  ll_val = ll_val_mle_rank1, rmse = rmse_mle_rank1)

```

### ebpmf
As expected `rank-1` gets optimal after the first iteration. 
```{r message=F, warning=F}
## rank 1 case
out_rank1 =  ebpmf::ebpmf_rank1_exponential_helper(X = sim$X, maxiter = 10, verbose = T)
lam_rank1 =  out_rank1$ql$mean  %*% t(out_rank1$qf$mean)
ll_train_rank1  = sum(dpois(sim$X, lam_rank1, log = T))
ll_val_rank1  = sum(dpois(sim$Y, lam_rank1, log = T))
rmse_rank1 = sqrt(mean((lam_rank1 - sim$Lam)^2))
data.frame(ll_train = ll_train_rank1,  ll_val = ll_val_rank1, rmse = rmse_rank1)
```




## rank  k

### ebpmf, fix g
In each iteration (> 2), the algorithm fixes $g$. 
```{r message=F, warning=F}
## rank-k: fix gl, gf after first iteration
out =  ebpmf::ebpmf_exponential_mixture(X = sim$X, K = K, maxiter.out = 100, verbose = F, fix_g = T)

lam_out =  out$qg$qls_mean  %*% t(out$qg$qfs_mean)
ll_train_out  = sum(dpois(sim$X, lam_out, log = T))
ll_val_out  = sum(dpois(sim$Y, lam_out, log = T))
rmse_out = sqrt(mean((lam_out - sim$Lam)^2))
data.frame(ll_train = ll_train_out,  ll_val = ll_val_out, rmse = rmse_out)

plot(out$ELBO, xlab = "iter", ylab = "ELBO")
```




<!-- ```{r warning=F} -->
<!-- rm(out) -->
<!-- ## rank-k: fix gl, gf after first iteration -->
<!-- iters = seq(1,20,1) -->
<!-- rmses = c() -->
<!-- for(iter in iters){ -->
<!--   out =  ebpmf::ebpmf_exponential_mixture(X = sim$X, K = K, maxiter.out = iter, verbose = F, fix_g = T) -->
<!--   lam_out = out$qg$qls_mean %*% t(out$qg$qfs_mean) -->
<!--   rmse_out = sqrt(mean((lam_out - sim$Lam)^2)) -->
<!--   rmses = c(rmses, rmse_out) -->
<!-- } -->

<!-- plot(out$ELBO) -->
<!-- # plot(rmses) -->
<!-- # plot(out$ll) -->
<!-- ``` -->




### ebpmf, fix scale 
In each iteration (>2), the algorithm fixes the scale (grid)
```{r message=F, warning=F}
rm(out)
## rank-k: fix scale after first iteration
out =  ebpmf::ebpmf_exponential_mixture(X = sim$X, K = K, maxiter.out = maxiter, verbose = F, fix_grid = T)

lam_out =  out$qg$qls_mean  %*% t(out$qg$qfs_mean)
ll_train_out  = sum(dpois(sim$X, lam_out, log = T))
ll_val_out  = sum(dpois(sim$Y, lam_out, log = T))
rmse_out = sqrt(mean((lam_out - sim$Lam)^2))
data.frame(ll_train = ll_train_out,  ll_val = ll_val_out, rmse = rmse_out)
plot(out$ELBO, xlab = "iter", ylab = "ELBO")
```



### ebpmf, free
In  each iteration, the algorithm estimates scale (grid)
```{r message=F, warning=F}
rm(out)
## rank-k: fix nothing
out =  ebpmf::ebpmf_exponential_mixture(X = sim$X, K = K, maxiter.out = maxiter, verbose = F,m = sqrt(2))

lam_out =  out$qg$qls_mean  %*% t(out$qg$qfs_mean)
ll_train_out  = sum(dpois(sim$X, lam_out, log = T))
ll_val_out  = sum(dpois(sim$Y, lam_out, log = T))
rmse_out = sqrt(mean((lam_out - sim$Lam)^2))
data.frame(ll_train = ll_train_out,  ll_val = ll_val_out, rmse = rmse_out)
plot(out$ELBO, xlab = "iter", ylab = "ELBO")
```

Now it is not strictly increasing. Let's use denser grids:
```{r message=F, warning=F}
rm(out)
## rank-k: fix nothing
out =  ebpmf::ebpmf_exponential_mixture(X = sim$X, K = K, maxiter.out = maxiter, verbose = F,m = 2^0.25)

lam_out =  out$qg$qls_mean  %*% t(out$qg$qfs_mean)
ll_train_out  = sum(dpois(sim$X, lam_out, log = T))
ll_val_out  = sum(dpois(sim$Y, lam_out, log = T))
rmse_out = sqrt(mean((lam_out - sim$Lam)^2))
data.frame(ll_train = ll_train_out,  ll_val = ll_val_out, rmse = rmse_out)
plot(out$ELBO, xlab = "iter", ylab = "ELBO")
```





### mle
```{r message=F, warning=F}
mle_rankK = NNLM::nnmf(A = sim$X, init = list(W0 = out$qg$qls_mean, H0 = t(out$qg$qfs_mean)),k = K, method = "lee", loss = "mkl", rel.tol = 1e-10, max.iter = 100)
lam_mle_rankK =  mle_rankK$W %*% mle_rankK$H
ll_train_mle_rankK  = sum(dpois(sim$X, lam_mle_rankK, log = T))
ll_val_mle_rankK  = sum(dpois(sim$Y, lam_mle_rankK, log = T))
rmse_mle_rankK = sqrt(mean((lam_mle_rankK - sim$Lam)^2))
data.frame(ll_train = ll_train_mle_rankK,  ll_val = ll_val_mle_rankK, rmse = rmse_mle_rankK)
```









