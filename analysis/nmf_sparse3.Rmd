---
title: "nmf_sparse3"
author: "zihao12"
date: "2019-11-03"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction
I continue to investigate the sparisty of solution  to the data in https://zihao12.github.io/ebpmf_demo/nmf_sparse.html, https://zihao12.github.io/ebpmf_demo/nmf_sparse2.html.  I have tried `ebpmf_point_gamma`, `ebpmf_exponential_mixture` and `ebpmf_exponential_mixture` with uniform weight on  each prior components. None of them works and ther resulting posterior mean are quite similar  to MLE result. \\

I look at the $Z$ from the initialization, and solves `ebpm` problem for $L$. The result is that all my methods get a result similar to MLE. By looking  at the $\hat{g}$ I find out why each method is similar to MLE result.  



```{r warning=F, message=F}
rm(list = ls())
devtools::load_all("../ebpmf.alpha/")
devtools::load_all("../ebpm/")
library("ggplot2")
library("NNLM") 
library("ebpmf")
n = 99
p = 300
iter_em = 1000
iter_eb = 100

KL <- function(true,est){
  sum(ifelse(true==0,0,true * log(true/est)) + est - true)
}

JS  <- function(true,est){
  0.5*(KL(true, est) + KL(est, true))
}

RMSE <- function(true, est){
  sqrt(mean((true - est)^2))
}

log_lik <- function(X, lam){
  return(sum(dpois(x = X, lambda = lam , log = T)))
}
# iter_em = 100
# iter_eb = 10
```

```{r}
set.seed(123)
k= 4
mfac = 2 # controls PVE of dense factor
L = matrix(0, nrow=n, ncol=k)
F = matrix(0, nrow=p, ncol=k)
L[1:(n/3),1] = 1
L[((n/3)+1):(2*n/3),2] = 1
L[((2*n/3)+1):n,3] = 1
L[,4] = 1+mfac*runif(n)
F[1:(p/3),1] = 1+10*runif(p/3)
F[((p/3)+1):(2*p/3),2] = 1+10*runif(p/3)
F[((2*p/3)+1):p,3] = 1+10*runif(p/3)
F[,4]= 1+mfac*runif(p)
lambda = L %*% t(F)
X = matrix(rpois(n=length(lambda),lambda),nrow=n)
image(X)
```

```{r}
fit_lee = NNLM::nnmf(A = X, k = k, loss = "mkl", method = "lee", max.iter = iter_em)
fit_scd = NNLM::nnmf(A = X, k = k, loss = "mkl", method = "scd", max.iter = iter_em)
qg_from_lee = initialize_qg_from_LF(L = fit_lee$W, F =  t(fit_lee$H))
qg_from_scd = initialize_qg_from_LF(L = fit_scd$W, F =  t(fit_scd$H))

Ez_from_lee  = get_Ez(X, qg_from_lee, k)
Ez_from_scd  = get_Ez(X, qg_from_scd, k)

for(i in 1:k){
 image(Ez_from_lee$Ez[,,i], main = sprintf("Ez[,,%d]", i)) 
}
```

let's take a look at Ez[,,1] and compute its row/column sum. First, it is not sparse at all. Second, there is a  jump of values in rumsum. 
```{r}
idx = 2
min(colSums(Ez_from_lee$Ez[,,idx]))
min(rowSums(Ez_from_lee$Ez[,,idx]))
hist(colSums(Ez_from_lee$Ez[,,idx]), breaks = 100)
hist(rowSums(Ez_from_lee$Ez[,,idx]), breaks = 100)

s= sum(qg_from_lee$qfs_mean[,idx])
print(s)
```

Let's apply `ebpm` to row/col sum and see what happens.\

First look  at the fit using `ebpm_exp` with and without uniform weight, and `ebpm_pg`. It  is clear the effect of shrinkage is very  small in all methods. 
```{r}
x = rowSums(Ez_from_lee$Ez[,,idx])
s = replicate(length(x),sum(qg_from_lee$qfs_mean[,idx]))

## there are some x that we hope to shrink towards 0
sort(x/s)[1:10]
hist(x/s, breaks = 100)

g_init = get_uniform_mixture(x = x, s= s, m = 2^0.25, low = 1e-20)
fit_l_exp_uniform = ebpm::ebpm_exponential_mixture(x = x, s = s, g_init = g_init, fix_g = T)
fit_l_exp = ebpm::ebpm_exponential_mixture(x = x, s = s)
fit_l_pg = ebpm::ebpm_point_gamma(x = x, s =  s)

#ix = sort(x/s, index.return = TRUE, decreasing = FALSE)$ix

df = data.frame(idx = 1:length(x), mle = x/s, exp_uniform = fit_l_exp_uniform$posterior$mean,
                exp = fit_l_exp$posterior$mean, pg = fit_l_pg$posterior$mean)

ggplot(df)+
  geom_point(aes(x = idx, y = mle, color = "mle"))+
  geom_line(aes(x = idx, y = pg, color = "pg"))+
  geom_line(aes(x = idx, y = exp, color = "exp"))+
  geom_line(aes(x = idx, y = exp_uniform, color = "exp_uniform"))+
  ylab("lambda")

ggplot(df)+
  geom_point(aes(x = mle, y = pg, color = "pg"))+
  geom_point(aes(x = mle, y = exp, color = "exp"))+
  geom_point(aes(x = mle, y = exp_uniform, color = "exp_uniform"))+
  geom_abline(slope = 1, intercept = 0)+
  ylab("fitted")
```

Take a closer look at smaller values
```{r}
df_small = df[df$mle < 2, ]
ggplot(df_small)+
  geom_point(aes(x = mle, y = pg, color = "pg"))+
  geom_point(aes(x = mle, y = exp, color = "exp"))+
  geom_point(aes(x = mle, y = exp_uniform, color = "exp_uniform"))+
  geom_abline(slope = 1, intercept = 0)+
  ylab("fitted")

df_big = df[df$mle > 3, ]
ggplot(df_big)+
  geom_point(aes(x = mle, y = pg, color = "pg"))+
  geom_point(aes(x = mle, y = exp, color = "exp"))+
  geom_point(aes(x = mle, y = exp_uniform, color = "exp_uniform"))+
  geom_abline(slope = 1, intercept = 0)+
  ylab("fitted")


## take a look at those small x/s
sort(df$mle)[1:10]
sort(df$exp_uniform)[1:10]
sort(df$exp)[1:10]
sort(df$pg)[1:10]
```
clearly `ebpm` fails to shrinkage them towards 0!

## Why our empirical bayes fails to  achieve sparisity. 
`ebpm_exp_uniform` does not achieve sparsity because the $L$ matrix is 0 for all those columns where the prior components are small (this makes sense as given the data it is unlikely to have a prior  near 0). As a result, $\tilde{\Pi}$ also has similar patterns. I show its image  (column  from left  to right should correspond scales from small to big)
```{r}
g = fit_l_exp_uniform$fitted_g
L = ebpm::compute_L(x, s, a = g$shape, b = 1/g$scale)$L
Pi_tilde = t(t(L) * g$pi)
Pi_tilde = Pi_tilde/rowSums(Pi_tilde)
dim(Pi_tilde)
image(t(Pi_tilde))
which.max(colSums(Pi_tilde))
fit_l_exp_uniform$fitted_g$scale[which.max(colSums(Pi_tilde))]
```

The posterior is a mixture of gamma of the form $Gamma(.;x + 1, s + b})$, with the posterior mean being $\frac{1}{1+b/s} (x/s) + (1- \frac{1}{1+b/s}) (a/b)$. Its most weights (posterior) are concentrated at the  prior component with big `scale` even if we fix $\pi$ to be uniform.  In this case, $s$ is large and `scale` is not too small, the result is very close to $x/s$. When $x/s$ is close to 0, posterior mean  is greater than $\frac{1}{s + 1/\text{scale}}$. The big value of `scale` prohibits it to be too small.  

`ebpm_exp` does not achieve sparsity for the same reason.   
```{r}
g = fit_l_exp$fitted_g
L = ebpm::compute_L(x, s, a = g$shape, b = 1/g$scale)$L
Pi_tilde = t(t(L) * g$pi)
Pi_tilde = Pi_tilde/rowSums(Pi_tilde)
dim(Pi_tilde)
which.max(colSums(Pi_tilde))
fit_l_exp$fitted_g$scale[which.max(colSums(Pi_tilde))]
image(t(Pi_tilde))

```

Let's try manually fixing very small prior components. Indeed this can drive all numbers down (except for very small  numbers). 
```{r}
g_init_manual = gammamix(pi = c(1,1,1)/3, shape = c(1,1, 1), scale = c(0.001,0.01, 12))
fit_l_exp_manual = ebpm::ebpm_exponential_mixture(x = x, s = s, g_init = g_init_manual, fix_g = T)
g = fit_l_exp_manual$fitted_g
L = ebpm::compute_L(x, s, a = g$shape, b = 1/g$scale)$L
Pi_tilde = t(t(L) * g$pi)
Pi_tilde = Pi_tilde/rowSums(Pi_tilde)
dim(Pi_tilde)
image(t(Pi_tilde))
## take a look at those small x/s
sort(x/s)[1:10]
sort(fit_l_exp_manual$posterior$mean)[1:10]

plot(x/s)
lines(fit_l_exp_manual$posterior$mean)
```

Try `ash_pois`
```{r}
library(ashr)
fit_ash = ashr::ash_pois(y =  x, scale = s)
plot(x/s)
lines(fit_ash$result$PosteriorMean)
```


`ebpm_point_gamma` does not achieve sparsity because the rowsums of $Z$ is not sparse, thus $\pi_0 = 0$. The posterior mean is just $\frac{x_i + \text{shape}}{s_i + 1/\text{scale}}$. Again,  it is very close to MLE. 
```{r}
g = fit_l_pg$fitted_g
g
```


## Try thresholding on  `Z`
From the simple experiment above we can see our current `ebpm` method does  not do very well when `Z` are not sparse. As a heuristic, I try to apply thresholding when updating `Z`: that is, for $\zeta_{ijk} < \text{threshold}$, I set it to 0 and get $\hat{\zeta}$ then normalize so that $\sum_k  \hat{\zeta}_{ijk} = 1$. 
```{r warning=F, message = F}
cutoff_rate =  c(0, 0.1,0.3,0.5,0.7, 0.9)
col_names = c()
for(rate in cutoff_rate){
  col_names = c(col_names, sprintf("cutoff_%.1f", rate))
}

# maxiter.out = iter_eb
# 
# summary_m = matrix(NA,nrow = 5, ncol = length(cutoff_rate))
# rownames(summary_m) = c("RMSE", "KL", "JS", "ELBO", "loglike")
# colnames(summary_m) = col_names
# 
# ELBO_m = matrix(NA,nrow = maxiter.out, ncol = length(cutoff_rate))
# colnames(ELBO_m) = col_names
# 
# loadings = matrix(NA,nrow = nrow(X), ncol = length(cutoff_rate))
# 
# for(i in 1:length(cutoff_rate)){
#   rate = cutoff_rate[i]
#   threshold = rate*1/k
#   fit_pg_threshold = ebpmf_point_gamma(X = X, K = k, qg = qg_from_lee, maxiter.out = maxiter.out, threshold = threshold)
#   ELBO_m[, i] = fit_pg_threshold$ELBO
#   lam_est = fit_pg_threshold$qg$qls_mean %*% t(fit_pg_threshold$qg$qfs_mean)
#   summary_ = c(RMSE(lambda, lam_est), KL(lambda, lam_est), JS(lambda, lam_est), fit_pg_threshold$ELBO[maxiter.out], log_lik(X, lam_est))
#   summary_m[,i] = summary_
#   loadings[,i] = fit_pg_threshold$qg$qls_mean[,2]
# }


# saveRDS(ELBO_m, "data/nmf_sparse3_elbo.Rds")
# saveRDS(loadings, "data/nmf_sparse3_loadings.Rds")
# saveRDS(summary_m, "data/nmf_sparse3_summary.Rds")

ELBO_m  = readRDS("data/nmf_sparse3_elbo.Rds")
loadings = readRDS("data/nmf_sparse3_loadings.Rds")
summary_m = readRDS("data/nmf_sparse3_summary.Rds")
```


```{r}
print(summary_m, digits = 4)
```


```{r}
df = data.frame(ELBO_m)
colnames(df) = col_names
df[["iter"]] = 1:nrow(df)
ggplot(df)+
  geom_point(aes(x = iter, y = cutoff_0.0, color = "cutoff_0.0"))+
  geom_point(aes(x = iter, y = cutoff_0.1, color = "cutoff_0.1"))+
  geom_point(aes(x = iter, y = cutoff_0.3, color = "cutoff_0.3"))+
  geom_point(aes(x = iter, y = cutoff_0.5, color = "cutoff_0.5"))+
  geom_point(aes(x = iter, y = cutoff_0.7, color = "cutoff_0.7"))+
  geom_point(aes(x = iter, y = cutoff_0.9, color = "cutoff_0.9"))+
  ylab("elbo")
```


```{r}
df = data.frame(loadings)
colnames(df) = col_names
df[["idx"]] = 1:nrow(df)

ggplot(df)+
  geom_point(aes(x = idx, y = cutoff_0.0, color = "cutoff_0.0"))+
  geom_point(aes(x = idx, y = cutoff_0.1, color = "cutoff_0.1"))+
  # geom_point(aes(x = idx, y = cutoff_0.3, color = "cutoff_0.3"))+
  # geom_point(aes(x = idx, y = cutoff_0.5, color = "cutoff_0.5"))+
  # geom_point(aes(x = idx, y = cutoff_0.7, color = "cutoff_0.7"))+
  # geom_point(aes(x = idx, y = cutoff_0.9, color = "cutoff_0.9"))+
  ylab("loading 2")
```






