---
title: "applications_kos"
author: "zihao12"
date: "2020-04-29"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction
* I applied Poisson Matrix Factorization to analyze a corpus of Daily Kos Blog dataset.\

* The data is downloaded from [Bag of Words](https://archive.ics.uci.edu/ml/datasets/Bag+of+Words). I run `NNLM::nnmf` for $1000$ iterations with $20$ topics, using `scd` algorithm.\

* I mainly look at $f_{jK}$ of the fitted model. \

* Note that, I use $L \frac{f_{jk}}{\sum_{j} f_{jk}}$ in place of $f_{jk}$ for better intrepretation. $L$ is the average document length. 
```{r}
rm(list = ls())
library(Matrix)
source("code/misc.R")
library(pheatmap)
set.seed(123)

data_dir = "~/Desktop/data/text"
data_name = "docword.kos.mtx"
model_name = "docword.kos_nnmf_K20_maxiter1000.Rds"
dict_name = "vocab.kos.txt"

Y = readMM(file= sprintf("%s/%s", data_dir, data_name))
Y = as.matrix(Y)
dict = read.csv(sprintf("%s/%s", data_dir, dict_name), header = FALSE)
dict = as.vector(dict[,1])
model = readRDS(sprintf("%s/%s", data_dir, model_name))
L = model$W
F = t(model$H)
s_k = colSums(L)

dim(L)
dim(F)

n = nrow(Y)
p = ncol(Y)
K = ncol(L)

## scale l,f into multinomial model
lf = poisson2multinom(F = F, L = L)
```

### scale `F`
```{r}
doc_length_med = median(rowSums(Y))
doc_length_med
F = t(t(F)/colSums(F))
F = doc_length_med * F
```



## look at the meaning of each topic 
let's look at the weight distribution in each topic (multinomial model). It seems that the top $0.002$ words take up most weight.
```{r fig.width=14, fig.height=14}
par(mfrow = c(5,4))
for(k in 1:K){
  probs = seq(0, 1, 0.002)
  #top_words = order(lf$F[,1], decreasing = TRUE)[1:13]
  plot(probs, quantile(lf$F[,k], probs = probs))
}
```

Below I show the top $0.002$ words in each topic (per column)
```{r}
n_word = round(0.002 * p)
topic_df <- matrix(,nrow = n_word, ncol = K)
for(k in 1:K){
  topic_df[,k] = dict[order(lf$F[,k], decreasing = TRUE)[1:n_word]]
}
colnames(topic_df) <- 1:K
topic_df
```



## Let's look at the structure of $f_{jk}$.

### most frequent words
Note that the most frequent words (like "a", "the")seem to have been eliminated ...
```{r fig.width=14, fig.height=14}
freq_order = order(colSums(Y)) ## increasing order

#idx = sample(x = 1:p, size = 9, replace = FALSE)
idx = freq_order[(p-15):p]
par(mfrow = c(4,4))
for(j in idx){
  plot(F[j,], xlab = "topic index", ylab = "f_jk",
       main = sprintf("f_jK for word `%s`", dict[j]))
}

```

### rarest words
Also note that words that occurred less than ten times are also eliminated
```{r fig.width=14, fig.height=14}
idx = freq_order[1:16]
par(mfrow = c(4,4))
for(j in idx){
  plot(F[j,], xlab = "topic index", ylab = "f_jk",
       main = sprintf("f_jK for word `%s`", dict[j]))
}
```

### other words
get some words that are neither most often nor rare 
```{r fig.width=14, fig.height=14}
idx = sample(x = 1:p, size = 16, replace = FALSE) ## hope it won't coincide with previous choices
par(mfrow = c(4,4))
for(j in idx){
  plot(F[j,], xlab = "topic index", ylab = "f_jk",
       main = sprintf("f_jK for word `%s`", dict[j]))
}
```

## most important words in a topic
```{r fig.width=14, fig.height=14}
par(mfrow = c(5,4))
for(k in 1:K){
  j = which.max(lf$F[,k])
  plot(F[j,], xlab = "topic index", ylab = "f_jk",
       main = sprintf("f_jK for `%s` in topic %d", dict[j], k))
}
```

look at top words using `pheatmap`
```{r fig.width=14, fig.height=14}
#par(mfrow = c(5,4))
n_top_word = round(0.002 * p)
for(k in 1:K){
  print(sprintf("topic %d", k))
  word_idx = order(lf$F[,k], decreasing = TRUE)[1:n_top_word]
  F_sub = F[word_idx, ]
  rownames(F_sub) = dict[word_idx]
  colnames(F_sub) = paste("Topic", 1:K, sep = "")
  pheatmap(F_sub)
}
```
