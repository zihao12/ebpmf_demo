---
title: "nmf_sparse2"
author: "zihao12"
date: "2019-10-30"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---
## Goal
Adapted from https://stephens999.github.io/misc/nmf_sparse.html
* This time we focus on `ebpmf_exponential_mixture` (`ebpmf_exp` as shorthand). The result depends heavily  on the initialization of those methods. So we use two initializatin methods: `lee` and `scd` in `NNLM::nnmf` function.  I will show the loadings from both initialization as well as the final `ebpmf` results to see how `ebpmf`  

* Besides the original `ebpmf_exp`, I fix the weights of the exponential mixture to be uniform. (In the first iteration, I choose the  grids as ususual (manually set the smallest scale to small number), and fix $\pi$ to be uniform; in the following iterations, the  $\hat{g}$s are fixed). In this way, most weights will be placed on exponentials with very small scales. I call it  `ebpmf_exp_uniform`. I also use two initialization methods.


```{r warning=F, message=F}
rm(list = ls())
devtools::load_all("../ebpmf.alpha/")
devtools::load_all("../ebpm/")
library("fastTopics")
library("NNLM") 
library("ebpmf")

n = 99
p = 300

iter_em = 10000
iter_eb = 100
# n = 50
# p = 100
```

## Sparse Loadings and Factors
The goal is to do some simple simulations where the factors are sparse
and look at the sparsity of the solutions from regular (unpenalized) nmf.

We simulate data with 3 factors with a "block-like"" structure.
```{r}
set.seed(123)
k= 3
L = matrix(0, nrow=n, ncol=k)
F = matrix(0, nrow=p, ncol=k)
L[1:(n/3),1] = 1
L[((n/3)+1):(2*n/3),2] = 1
L[((2*n/3)+1):n,3] = 1
  
F[1:(p/3),1] = 1+10*runif(p/3)
F[((p/3)+1):(2*p/3),2] = 1+10*runif(p/3)
F[((2*p/3)+1):p,3] = 1+10*runif(p/3)
lambda = L %*% t(F)
X = matrix(rpois(n=length(lambda),lambda),nrow=n)
image(X)
```

Now run the methods, and compute the Poisson log-likelihoods.
```{r warning=F, message=F,echo=FALSE}
k=3
fit_lee = NNLM::nnmf(A = X, k = k, loss = "mkl", method = "lee", max.iter = iter_em)
## scd
fit_scd = NNLM::nnmf(A = X, k = k, loss = "mkl", method = "scd", max.iter = iter_em)

qg_from_lee = initialize_qg_from_LF(L = fit_lee$W, F =  t(fit_lee$H))
qg_from_scd = initialize_qg_from_LF(L = fit_scd$W, F =  t(fit_scd$H))

fit_ebpmf_exp_lee = ebpmf_exponential_mixture(X, K = k, qg = qg_from_lee, maxiter.out = iter_eb)
fit_ebpmf_exp_scd = ebpmf_exponential_mixture(X, K = k, qg = qg_from_scd, maxiter.out = iter_eb)



#fit_ebpmf_exp_uniform =  ebpmf_exponential_mixture_uniform(X, K = k, m = 2^0.25,maxiter.out = 100, init_method = "lee", low = 1e-30)
```

Let's see the training loglikelihood
```{r}
sum(dpois(X, fit_lee$W %*% fit_lee$H,log=TRUE))
sum(dpois(X, fit_scd$W %*% fit_scd$H,log=TRUE))
#sum(dpois(X, fit_ebpmf_pg$qg$qls_mean %*% t(fit_ebpmf_pg$qg$qfs_mean),log=TRUE))
sum(dpois(X, fit_ebpmf_exp_lee$qg$qls_mean %*% t(fit_ebpmf_exp_lee$qg$qfs_mean),log=TRUE))

sum(dpois(X, fit_ebpmf_exp_scd$qg$qls_mean %*% t(fit_ebpmf_exp_scd$qg$qfs_mean),log=TRUE))


#sum(dpois(X, fit_ebpmf_exp_uniform$qg$qls_mean %*% t(fit_ebpmf_exp_uniform$qg$qfs_mean),log=TRUE))
## oracle
sum(dpois(X,lambda ,log=TRUE))
```


So all MLE algorithms find the same solution. `ebpmf_pg` also exceeds oracle.

Let's look at a factor/loading: the results are highly sparse.
```{r}
## fit_lee
method = "lee"
for(d in 1:k){
  plot(fit_lee$W[,d],main=sprintf("%s: estimated loadings %d", method, d))
}

## fit_scd
method = "scd"
for(d in 1:k){
  plot(fit_scd$W[,d],main=sprintf("%s: estimated loadings %d", method, d))
}

## fit_ebpmf_exp_lee
method = "ebpmf_exp_lee"
for(d in 1:k){
  plot(fit_ebpmf_exp_lee$qg$qls_mean[,d],main=sprintf("%s: estimated loadings %d", method, d))
}

## fit_ebpmf_exp_scd
method = "ebpmf_exp_scd"
for(d in 1:k){
  plot(fit_ebpmf_exp_scd$qg$qls_mean[,d],main=sprintf("%s: estimated loadings %d", method, d))
}
```


## Add a dense fourth factor

Now I add a fourth factor that is dense.
Note that we can make the problem much harder
by making the 4th (dense) factor have larger PVE (increase `mfac` in the code).
That may be useful for comparing methods on a harder problem...
```{r}
set.seed(123)
k= 4
mfac = 2 # controls PVE of dense factor
L = matrix(0, nrow=n, ncol=k)
F = matrix(0, nrow=p, ncol=k)
L[1:(n/3),1] = 1
L[((n/3)+1):(2*n/3),2] = 1
L[((2*n/3)+1):n,3] = 1
L[,4] = 1+mfac*runif(n)
F[1:(p/3),1] = 1+10*runif(p/3)
F[((p/3)+1):(2*p/3),2] = 1+10*runif(p/3)
F[((2*p/3)+1):p,3] = 1+10*runif(p/3)
F[,4]= 1+mfac*runif(p)
lambda = L %*% t(F)
X = matrix(rpois(n=length(lambda),lambda),nrow=n)
image(X)
```



Run the methods. 
```{r warning=F, message=F, echo=FALSE}
k=4
fit_lee = NNLM::nnmf(A = X, k = k, loss = "mkl", method = "lee", max.iter = iter_em)
## scd
fit_scd = NNLM::nnmf(A = X, k = k, loss = "mkl", method = "scd", max.iter = iter_em)

qg_from_lee = initialize_qg_from_LF(L = fit_lee$W, F =  t(fit_lee$H))
qg_from_scd = initialize_qg_from_LF(L = fit_scd$W, F =  t(fit_scd$H))

fit_ebpmf_exp_lee = ebpmf_exponential_mixture(X, K = k, qg = qg_from_lee, m = 2^0.25, maxiter.out = iter_eb)
fit_ebpmf_exp_scd = ebpmf_exponential_mixture(X, K = k, qg = qg_from_scd, m = 2^0.25, maxiter.out = iter_eb)

fit_ebpmf_exp_uniform_lee =  ebpmf_exponential_mixture_uniform(X, K = k, qg = qg_from_lee, m = 2^0.25,maxiter.out = iter_eb, init_method = "lee", low = 1e-30)
fit_ebpmf_exp_uniform_scd =  ebpmf_exponential_mixture_uniform(X, K = k, qg = qg_from_scd, m = 2^0.25,maxiter.out = iter_eb, init_method = "lee", low = 1e-30)
```


```{r}
sum(dpois(X, fit_lee$W %*% fit_lee$H,log=TRUE))
sum(dpois(X, fit_scd$W %*% fit_scd$H,log=TRUE))
#sum(dpois(X, fit_ebpmf_pg$qg$qls_mean %*% t(fit_ebpmf_pg$qg$qfs_mean),log=TRUE))
sum(dpois(X, fit_ebpmf_exp_lee$qg$qls_mean %*% t(fit_ebpmf_exp_lee$qg$qfs_mean),log=TRUE))

sum(dpois(X, fit_ebpmf_exp_scd$qg$qls_mean %*% t(fit_ebpmf_exp_scd$qg$qfs_mean),log=TRUE))

sum(dpois(X, fit_ebpmf_exp_uniform_lee$qg$qls_mean %*% t(fit_ebpmf_exp_uniform_lee$qg$qfs_mean),log=TRUE))

sum(dpois(X, fit_ebpmf_exp_uniform_scd$qg$qls_mean %*% t(fit_ebpmf_exp_uniform_scd$qg$qfs_mean),log=TRUE))

#sum(dpois(X, fit_ebpmf_exp_uniform$qg$qls_mean %*% t(fit_ebpmf_exp_uniform$qg$qfs_mean),log=TRUE))
## oracle
sum(dpois(X,lambda ,log=TRUE))
```

All the methods find a solution and loglikelihood exceeds the oracle.

Let's  see  if we can get a cleaner loading by fixing much weights on smaller values for `ebpmf_exp`. 
```{r}
## fit_lee
method = "lee"
for(d in 1:k){
  plot(fit_lee$W[,d],main=sprintf("%s: estimated loadings %d", method, d))
}

## fit_scd
method = "scd"
for(d in 1:k){
  plot(fit_scd$W[,d],main=sprintf("%s: estimated loadings %d", method, d))
}

## fit_ebpmf_exp_lee
method = "ebpmf_exp_lee"
for(d in 1:k){
  plot(fit_ebpmf_exp_lee$qg$qls_mean[,d],main=sprintf("%s: estimated loadings %d", method, d))
}

## fit_ebpmf_exp_scd
method = "ebpmf_exp_scd"
for(d in 1:k){
  plot(fit_ebpmf_exp_scd$qg$qls_mean[,d],main=sprintf("%s: estimated loadings %d", method, d))
}

## fit_ebpmf_exp_lee
method = "ebpmf_exp_uniform_lee"
for(d in 1:k){
  plot(fit_ebpmf_exp_uniform_lee$qg$qls_mean[,d],main=sprintf("%s: estimated loadings %d", method, d))
}

## fit_ebpmf_exp_scd
method = "ebpmf_exp_uniform_scd"
for(d in 1:k){
  plot(fit_ebpmf_exp_uniform_scd$qg$qls_mean[,d],main=sprintf("%s: estimated loadings %d", method, d))
}
```





Compute the KL divergence between $\lambda_{true}$ and $\lambda_{est}$. 
```{r}
# compute goodness of fit to true lambda
KL = function(true,est){
  sum(ifelse(true==0,0,true * log(true/est)) + est - true)
}
get_WH_nnmf= function(fit){fit$W %*% fit$H}
get_WH_ebpmf= function(fit){fit$qg$qls_mean %*% t(fit$qg$qfs_mean)}
```

```{r}
KL(lambda,get_WH_nnmf(fit_lee))

KL(lambda,get_WH_nnmf(fit_scd))

KL(lambda,get_WH_ebpmf(fit_ebpmf_exp_lee))

KL(lambda,get_WH_ebpmf(fit_ebpmf_exp_scd))

KL(lambda,get_WH_ebpmf(fit_ebpmf_exp_uniform_lee))

KL(lambda,get_WH_ebpmf(fit_ebpmf_exp_uniform_scd))
```














