---
title: "experiment_nb_means"
author: "zihao12"
date: "2020-03-18"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Model

\begin{align}
    & y_{jk} \sim Pois(s_k \mu_j v_{jk})\\
    & v_{jk} \sim  Ga(1/\phi_{jk}, 1/\phi_{jk})\\
\end{align}

The assumption is that $\Phi$ is sparse. \

The model and implementation details are in [another poisson means](https://zihao12.github.io/ebpmf_demo/another_poisson_means.pdf). The code is in [code](https://github.com/zihao12/ebpmf_demo/blob/master/script/nb_means.R)

```{r}
rm(list = ls())
source("script/nb_means.R")
set.seed(123)
```

Below I simulate $y_{jk}$ from the model above. I run my algorithm 3 times: random initialization twice, and initialization from truth once. 

```{r}
exper_nb_means <- function(K = 3, p = 999, maxiter = 10, seed = 123){
  set.seed(seed)
  eps = 1e-3
  signal = 10
  s = replicate(K, 1)
  mu = runif(p) 
  Phi = matrix(eps * runif(p * K), nrow = p, ncol = K)
  Phi[1:(p/3),1] = 1 + signal * runif(p/3)
  Phi[((p/3)+1):(2*p/3), 2] = 1 + signal * runif(p/3)
  Phi[((2*p/3)+1):p,3] = 1 + signal * runif(p/3)
  A = 1/Phi
  
  ## simulate data from the model
  V = matrix(rgamma(n = p*K, shape = A, rate = A), nrow = p)
  Theta = (mu %o% s) * V
  Y = matrix(rpois(n = p * K, lambda = Theta), nrow = p)
  
  ll_oracle = loglikelihood.nb_means(Y, s, mu, A)
  #print(sprintf("ll_oracle : %f", ll_oracle))
  
  mu0 = runif(p) 
  A0 = matrix(runif(p * K), nrow = p, ncol = K)
  runtime <- system.time(
    fit_init_random1 <- mle.nb_means.workhorse(Y, s, mu0, A0, maxiter = maxiter, verbose = FALSE,
                              control = list(gradient = TRUE, hessian = FALSE))
  )
  fit_init_random1[["runtime"]] = runtime[[3]]

  mu0 = runif(p) 
  A0 = matrix(runif(p * K), nrow = p, ncol = K)
  runtime <- system.time(
    fit_init_random2 <- mle.nb_means.workhorse(Y, s, mu0, A0, maxiter = maxiter, verbose = FALSE,
                              control = list(gradient = TRUE, hessian = FALSE))
  )
  fit_init_random2[["runtime"]] = runtime[[3]]
  
  runtime <- system.time(
    fit_init_truth <- mle.nb_means.workhorse(Y, s, mu, A, maxiter = maxiter, verbose = FALSE,
                              control = list(gradient = TRUE, hessian = FALSE))
  )
  fit_init_truth[["runtime"]] = runtime[[3]]
  
  return(list(Y = Y, mu = mu, Phi = Phi, ll_oracle = ll_oracle, 
  fit_init_random1 = fit_init_random1, fit_init_random2 = fit_init_random2, fit_init_truth = fit_init_truth))
}

```


```{r cache=TRUE, autodep=TRUE, message=FALSE, warning=FALSE}
exper <- exper_nb_means(K = 3, p = 999, maxiter = 100)
```



## comparee loglikelihood
```{r}
plot(exper$fit_init_random2$progress, col = "red", xlab  = "niter", ylab = "loglikelihood", type = "l")
lines(exper$fit_init_random1$progress, col = "green")
lines(exper$fit_init_truth$progress, col = "blue")
abline(h = exper$ll_oracle, col = "black")
legend("bottomright", legend=c("init from random2", "init from random1", "init from truth", "truth"),
       col=c("red","green" ,"blue", "black"), lty=c(1,1,1,1), cex=0.8)
```

## compare $\Phi$

```{r}
K = 3
for(k in 1:K){
  par(mfrow = c(2,2))
  plot(exper$Phi[,k], ylab = sprintf("truth Phi[,%d]", k))
  plot(1/exper$fit_init_random1$A[,k], ylab = sprintf("init from random1 Phi[,%d]", k))
  plot(1/exper$fit_init_random2$A[,k], ylab = sprintf("init from random2 Phi[,%d]", k))
  plot(1/exper$fit_init_truth$A[,k], ylab = sprintf("init from truth Phi[,%d]", k))
}
```

## runtime
```{r}
exper$fit_init_random1$runtime
```
So around 1 second/iteration, for $p = 999, K = 3$ (note that I restrict `nlm` steps to be $5$). 


## Conclusion
* The result is very dependent on initiailization. \

* The algorithm does not recover the true structure in $\Phi$ well. 


