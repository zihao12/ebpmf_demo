---
title: "experiment_ebpm_bg"
author: "zihao12"
date: "2020-04-24"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction
* Model

\begin{align}
  & Y_{jk} \sim Pois(s_k \lambda_{jk})\\
  & \lambda_{jk} = \mu_j v_{jk}\\
  & v_{jk} \sim \sum_l \Pi_{lk} Ga(a_l, a_l)
\end{align}

where $Y_{jk}$, $s_k$ are known & $\{a_l\}_{l}$ are prespecified (like $10^{-10}, 10^{-9} ... 10^9, 10^{10}$). 

* We call it background model because we model $\lambda_{jk} = \mu_j v_{jk}$ where $\mu_j$ is the gene-wise background. With this model, we hope to be able to identify those gene-sample pair that deviates from the background mean, and better shrink $\lambda_{jk}$. \

* Also we note that it is very similar to [ebpm-gamma-mixture](https://github.com/stephenslab/ebpm/blob/master/R/ebpm_gamma_mixture.R). Before the difficulty with `ebpm-gamma-mixture` is that we can't get good grids for $\lambda_{jk}$. Here we estimate grids (through estimating $\mu_j$) and weights alternatingly. Although we have a broader defintion of "sparsity" (in terms of deviation from background) in mind here, $Ga(1/a, 1/a)$ that has large variance has most weights on 0, so it is possible we can achieve sparsity in $\lambda_{jk}$ here as well.


## data simulation
I simulate $v_{jk} \sim Ga(1/\phi_{jk}, 1/\phi_{jk})$ (so $var(v_{jk}) = \phi_{jk}$), with 80% $\phi_{jk} = 10^{-4}$, 10% $\phi_{jk} = 10^{+4}$, 10% $\phi_{jk} = 10^{+8}$. Seems that most bigger $\phi$ yields $v = 0$. So the outlier (20%) for each $j$ are mostly 0. 
```{r}
rm(list = ls())
source("script/ebpm_background2.R")
source("script/nb_means.R")
set.seed(123)

K = 10
p = 999
eps = 1e-3
signal = 10
s = replicate(K, 1)

mu = 10 + 100*runif(p)
Phi = runif(n = p*K) ## now Phi is just index
mask_small = Phi < 0.8
Phi[mask_small] = 1e-4
id_tmp = rbinom(sum(!mask_small),1, 0.5)
Phi[!mask_small] = id_tmp * 1e+4 + (1- id_tmp) * 1e+8
Phi = matrix(Phi, nrow = p)
A = 1/Phi


## simulate data from the model
V = matrix(rgamma(n = p*K, shape = A, rate = A), nrow = p)
Lam = (mu %o% s) * V
Y = matrix(rpois(n = p * K, lambda = Lam), nrow = p)
```

## fit with background model (both EB and MLE approach)
```{r cache=TRUE, autodep=TRUE, message=FALSE, warning=FALSE}
maxiter = 100

runtime_bg_eb <- system.time(
  fit_bg_eb <- ebpm_background(Y = Y, s = s, maxiter = maxiter)
)

```


```{r}
## runtime
runtime_bg_eb
## algo progress
plot(fit_bg_eb$progress, xlab = "niter", ylab = "log-likelihood", type ="l")
```

## look at data
```{r}
par(mfrow=c(2,2))
hist(mu, breaks = 100)

hist(V, breaks = 100)

hist(Y, breaks  = 100)
```



## compare fits
```{r}
lam_mle = t(t(Y)/s)
lam_bg_eb = fit_bg_eb$mu * fit_bg_eb$posterior$mean

plot(Lam, lam_mle, col = "black", pch = 18, xlab = "lam_true", ylab = "lam_fit")
points(Lam, lam_bg_eb, col = "red", pch = 16)
legend("bottomright", legend=c("lam_mle", "lam_background_eb"),
       col=c("black", "red"), lty=c(1,1), cex=0.8)
## compare RMSE
mean((Lam - lam_mle)^2)
mean((Lam - lam_bg_eb)^2)
```

### look at $\mu_j$
```{r}
plot(mu, fit_bg_eb$mu, col = "red", pch = 16, ylab = "mu_fitted")
```

### look at $V_{jk}$
```{r}
V_bg_eb = fit_bg_eb$posterior$mean
plot(V, V_bg_eb, col = "red", pch = 16)

## look at V_bg_eb more closely
mask <- (V > 0.5)
plot(V[mask], V_bg_eb[mask], col = "red", pch = 16)
```

## look at how it is shrunk
```{r}
idx = 3
Y[idx,]
plot(Y[idx,], s * lam_bg_eb[idx, ])
```

